{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *随机 W 会导致各个类别权重相差不大。那么每个类别的概率就在 10% 左右*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.198244 analytic: -0.198244, relative error: 1.746713e-08\n",
      "numerical: -1.613102 analytic: -1.613102, relative error: 7.749639e-09\n",
      "numerical: -0.342798 analytic: -0.342798, relative error: 3.255915e-08\n",
      "numerical: -3.395965 analytic: -3.395965, relative error: 1.416008e-08\n",
      "numerical: 0.101489 analytic: 0.101489, relative error: 1.407656e-07\n",
      "numerical: 0.049143 analytic: 0.049143, relative error: 7.205831e-07\n",
      "numerical: 0.140973 analytic: 0.140973, relative error: 2.130369e-07\n",
      "numerical: -0.930270 analytic: -0.930271, relative error: 5.036234e-08\n",
      "numerical: 0.985119 analytic: 0.985119, relative error: 3.754149e-08\n",
      "numerical: -0.563908 analytic: -0.563908, relative error: 2.838528e-08\n",
      "numerical: -2.307200 analytic: -2.307200, relative error: 1.760601e-08\n",
      "numerical: -1.317386 analytic: -1.317386, relative error: 3.292841e-08\n",
      "numerical: -1.841551 analytic: -1.841551, relative error: 2.341026e-08\n",
      "numerical: 1.242678 analytic: 1.242678, relative error: 3.846591e-08\n",
      "numerical: -0.912561 analytic: -0.912561, relative error: 1.778213e-08\n",
      "numerical: -0.616471 analytic: -0.616471, relative error: 5.363038e-09\n",
      "numerical: 0.006396 analytic: 0.006396, relative error: 1.963375e-06\n",
      "numerical: -3.197728 analytic: -3.197728, relative error: 1.500194e-08\n",
      "numerical: -1.603462 analytic: -1.603462, relative error: 2.414652e-08\n",
      "numerical: 4.972279 analytic: 4.972279, relative error: 5.673059e-09\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.384375e+00 computed in 0.170447s\n",
      "vectorized loss: 2.384375e+00 computed in 0.021129s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 700: loss 766.322810\n",
      "iteration 100 / 700: loss 281.399166\n",
      "iteration 200 / 700: loss 104.208477\n",
      "iteration 300 / 700: loss 39.477204\n",
      "iteration 400 / 700: loss 15.722916\n",
      "iteration 500 / 700: loss 7.081407\n",
      "iteration 600 / 700: loss 3.927738\n",
      "iteration 0 / 700: loss 1786.413525\n",
      "iteration 100 / 700: loss 173.329044\n",
      "iteration 200 / 700: loss 18.575729\n",
      "iteration 300 / 700: loss 3.701213\n",
      "iteration 400 / 700: loss 2.299255\n",
      "iteration 500 / 700: loss 2.170119\n",
      "iteration 600 / 700: loss 2.163255\n",
      "iteration 0 / 700: loss 4159.055304\n",
      "iteration 100 / 700: loss 19.325894\n",
      "iteration 200 / 700: loss 2.291314\n",
      "iteration 300 / 700: loss 2.215283\n",
      "iteration 400 / 700: loss 2.239115\n",
      "iteration 500 / 700: loss 2.185543\n",
      "iteration 600 / 700: loss 2.209198\n",
      "iteration 0 / 700: loss 9576.176030\n",
      "iteration 100 / 700: loss 2.273030\n",
      "iteration 200 / 700: loss 2.256304\n",
      "iteration 300 / 700: loss 2.252244\n",
      "iteration 400 / 700: loss 2.249434\n",
      "iteration 500 / 700: loss 2.242549\n",
      "iteration 600 / 700: loss 2.236695\n",
      "iteration 0 / 700: loss 22491.375465\n",
      "iteration 100 / 700: loss 2.272533\n",
      "iteration 200 / 700: loss 2.288940\n",
      "iteration 300 / 700: loss 2.286686\n",
      "iteration 400 / 700: loss 2.274493\n",
      "iteration 500 / 700: loss 2.285542\n",
      "iteration 600 / 700: loss 2.262325\n",
      "iteration 0 / 700: loss 52501.501630\n",
      "iteration 100 / 700: loss 2.293552\n",
      "iteration 200 / 700: loss 2.293860\n",
      "iteration 300 / 700: loss 2.293639\n",
      "iteration 400 / 700: loss 2.289103\n",
      "iteration 500 / 700: loss 2.295466\n",
      "iteration 600 / 700: loss 2.294869\n",
      "iteration 0 / 700: loss 122541.430023\n",
      "iteration 100 / 700: loss 2.302588\n",
      "iteration 200 / 700: loss 2.297913\n",
      "iteration 300 / 700: loss 2.297542\n",
      "iteration 400 / 700: loss 2.300729\n",
      "iteration 500 / 700: loss 2.297730\n",
      "iteration 600 / 700: loss 2.297447\n",
      "iteration 0 / 700: loss 280827.344503\n",
      "iteration 100 / 700: loss 2.315895\n",
      "iteration 200 / 700: loss 2.325635\n",
      "iteration 300 / 700: loss 2.318375\n",
      "iteration 400 / 700: loss 2.318538\n",
      "iteration 500 / 700: loss 2.317012\n",
      "iteration 600 / 700: loss 2.323200\n",
      "iteration 0 / 700: loss 665243.742940\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1547139.169668\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 767.270780\n",
      "iteration 100 / 700: loss 258.007314\n",
      "iteration 200 / 700: loss 87.690057\n",
      "iteration 300 / 700: loss 30.718122\n",
      "iteration 400 / 700: loss 11.667862\n",
      "iteration 500 / 700: loss 5.310805\n",
      "iteration 600 / 700: loss 3.204722\n",
      "iteration 0 / 700: loss 1799.200461\n",
      "iteration 100 / 700: loss 142.176211\n",
      "iteration 200 / 700: loss 13.081985\n",
      "iteration 300 / 700: loss 2.996186\n",
      "iteration 400 / 700: loss 2.205315\n",
      "iteration 500 / 700: loss 2.138395\n",
      "iteration 600 / 700: loss 2.176983\n",
      "iteration 0 / 700: loss 4169.139342\n",
      "iteration 100 / 700: loss 12.639678\n",
      "iteration 200 / 700: loss 2.255706\n",
      "iteration 300 / 700: loss 2.218531\n",
      "iteration 400 / 700: loss 2.227506\n",
      "iteration 500 / 700: loss 2.197212\n",
      "iteration 600 / 700: loss 2.212227\n",
      "iteration 0 / 700: loss 9635.439141\n",
      "iteration 100 / 700: loss 2.254274\n",
      "iteration 200 / 700: loss 2.246396\n",
      "iteration 300 / 700: loss 2.265305\n",
      "iteration 400 / 700: loss 2.253710\n",
      "iteration 500 / 700: loss 2.252020\n",
      "iteration 600 / 700: loss 2.252168\n",
      "iteration 0 / 700: loss 22484.761447\n",
      "iteration 100 / 700: loss 2.289228\n",
      "iteration 200 / 700: loss 2.272296\n",
      "iteration 300 / 700: loss 2.269484\n",
      "iteration 400 / 700: loss 2.273129\n",
      "iteration 500 / 700: loss 2.272265\n",
      "iteration 600 / 700: loss 2.277032\n",
      "iteration 0 / 700: loss 52974.085851\n",
      "iteration 100 / 700: loss 2.292081\n",
      "iteration 200 / 700: loss 2.293232\n",
      "iteration 300 / 700: loss 2.291186\n",
      "iteration 400 / 700: loss 2.292494\n",
      "iteration 500 / 700: loss 2.294116\n",
      "iteration 600 / 700: loss 2.299004\n",
      "iteration 0 / 700: loss 120434.651388\n",
      "iteration 100 / 700: loss 2.300076\n",
      "iteration 200 / 700: loss 2.299376\n",
      "iteration 300 / 700: loss 2.303748\n",
      "iteration 400 / 700: loss 2.300980\n",
      "iteration 500 / 700: loss 2.297876\n",
      "iteration 600 / 700: loss 2.301426\n",
      "iteration 0 / 700: loss 282206.655684\n",
      "iteration 100 / 700: loss 2126287.221473\n",
      "iteration 200 / 700: loss 16003644.315414\n",
      "iteration 300 / 700: loss inf\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 654894.827075\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1535752.738725\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 769.879154\n",
      "iteration 100 / 700: loss 234.811442\n",
      "iteration 200 / 700: loss 72.725882\n",
      "iteration 300 / 700: loss 23.537213\n",
      "iteration 400 / 700: loss 8.628820\n",
      "iteration 500 / 700: loss 4.046805\n",
      "iteration 600 / 700: loss 2.623196\n",
      "iteration 0 / 700: loss 1779.692122\n",
      "iteration 100 / 700: loss 112.409083\n",
      "iteration 200 / 700: loss 9.000766\n",
      "iteration 300 / 700: loss 2.574623\n",
      "iteration 400 / 700: loss 2.195223\n",
      "iteration 500 / 700: loss 2.133951\n",
      "iteration 600 / 700: loss 2.156281\n",
      "iteration 0 / 700: loss 4201.822325\n",
      "iteration 100 / 700: loss 8.374347\n",
      "iteration 200 / 700: loss 2.190758\n",
      "iteration 300 / 700: loss 2.232212\n",
      "iteration 400 / 700: loss 2.241582\n",
      "iteration 500 / 700: loss 2.238605\n",
      "iteration 600 / 700: loss 2.219260\n",
      "iteration 0 / 700: loss 9711.323335\n",
      "iteration 100 / 700: loss 2.250039\n",
      "iteration 200 / 700: loss 2.247477\n",
      "iteration 300 / 700: loss 2.251491\n",
      "iteration 400 / 700: loss 2.245709\n",
      "iteration 500 / 700: loss 2.254655\n",
      "iteration 600 / 700: loss 2.254867\n",
      "iteration 0 / 700: loss 22295.271176\n",
      "iteration 100 / 700: loss 2.272732\n",
      "iteration 200 / 700: loss 2.281347\n",
      "iteration 300 / 700: loss 2.282587\n",
      "iteration 400 / 700: loss 2.273960\n",
      "iteration 500 / 700: loss 2.272649\n",
      "iteration 600 / 700: loss 2.276454\n",
      "iteration 0 / 700: loss 52437.865937\n",
      "iteration 100 / 700: loss 2.290438\n",
      "iteration 200 / 700: loss 2.289080\n",
      "iteration 300 / 700: loss 2.293340\n",
      "iteration 400 / 700: loss 2.291874\n",
      "iteration 500 / 700: loss 2.296543\n",
      "iteration 600 / 700: loss 2.292347\n",
      "iteration 0 / 700: loss 120713.317910\n",
      "iteration 100 / 700: loss 2.301497\n",
      "iteration 200 / 700: loss 2.302377\n",
      "iteration 300 / 700: loss 2.303076\n",
      "iteration 400 / 700: loss 2.299130\n",
      "iteration 500 / 700: loss 2.301061\n",
      "iteration 600 / 700: loss 2.301164\n",
      "iteration 0 / 700: loss 284236.710664\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 657566.226658\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1516414.147727\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 778.333190\n",
      "iteration 100 / 700: loss 213.917675\n",
      "iteration 200 / 700: loss 59.946110\n",
      "iteration 300 / 700: loss 17.860406\n",
      "iteration 400 / 700: loss 6.356559\n",
      "iteration 500 / 700: loss 3.265616\n",
      "iteration 600 / 700: loss 2.448755\n",
      "iteration 0 / 700: loss 1791.869083\n",
      "iteration 100 / 700: loss 88.866387\n",
      "iteration 200 / 700: loss 6.390926\n",
      "iteration 300 / 700: loss 2.375310\n",
      "iteration 400 / 700: loss 2.166587\n",
      "iteration 500 / 700: loss 2.206151\n",
      "iteration 600 / 700: loss 2.124439\n",
      "iteration 0 / 700: loss 4172.051202\n",
      "iteration 100 / 700: loss 5.651726\n",
      "iteration 200 / 700: loss 2.238720\n",
      "iteration 300 / 700: loss 2.228051\n",
      "iteration 400 / 700: loss 2.186742\n",
      "iteration 500 / 700: loss 2.214378\n",
      "iteration 600 / 700: loss 2.190211\n",
      "iteration 0 / 700: loss 9728.853338\n",
      "iteration 100 / 700: loss 2.256734\n",
      "iteration 200 / 700: loss 2.258248\n",
      "iteration 300 / 700: loss 2.252638\n",
      "iteration 400 / 700: loss 2.244567\n",
      "iteration 500 / 700: loss 2.267951\n",
      "iteration 600 / 700: loss 2.251274\n",
      "iteration 0 / 700: loss 22757.242310\n",
      "iteration 100 / 700: loss 2.280387\n",
      "iteration 200 / 700: loss 2.279826\n",
      "iteration 300 / 700: loss 2.289731\n",
      "iteration 400 / 700: loss 2.276458\n",
      "iteration 500 / 700: loss 2.280572\n",
      "iteration 600 / 700: loss 2.281902\n",
      "iteration 0 / 700: loss 51864.350499\n",
      "iteration 100 / 700: loss 2.292594\n",
      "iteration 200 / 700: loss 2.292303\n",
      "iteration 300 / 700: loss 2.297112\n",
      "iteration 400 / 700: loss 2.289840\n",
      "iteration 500 / 700: loss 2.289538\n",
      "iteration 600 / 700: loss 2.293128\n",
      "iteration 0 / 700: loss 123574.056914\n",
      "iteration 100 / 700: loss 2.302186\n",
      "iteration 200 / 700: loss 2.300101\n",
      "iteration 300 / 700: loss 2.300541\n",
      "iteration 400 / 700: loss 2.301527\n",
      "iteration 500 / 700: loss 2.300682\n",
      "iteration 600 / 700: loss 2.301331\n",
      "iteration 0 / 700: loss 283312.498192\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 661197.131280\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1537292.483111\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 774.446490\n",
      "iteration 100 / 700: loss 190.019116\n",
      "iteration 200 / 700: loss 47.851490\n",
      "iteration 300 / 700: loss 13.246906\n",
      "iteration 400 / 700: loss 4.832024\n",
      "iteration 500 / 700: loss 2.767330\n",
      "iteration 600 / 700: loss 2.240497\n",
      "iteration 0 / 700: loss 1798.291508\n",
      "iteration 100 / 700: loss 68.519715\n",
      "iteration 200 / 700: loss 4.617450\n",
      "iteration 300 / 700: loss 2.198037\n",
      "iteration 400 / 700: loss 2.167878\n",
      "iteration 500 / 700: loss 2.155704\n",
      "iteration 600 / 700: loss 2.164489\n",
      "iteration 0 / 700: loss 4120.777139\n",
      "iteration 100 / 700: loss 3.977107\n",
      "iteration 200 / 700: loss 2.219762\n",
      "iteration 300 / 700: loss 2.204456\n",
      "iteration 400 / 700: loss 2.216809\n",
      "iteration 500 / 700: loss 2.212419\n",
      "iteration 600 / 700: loss 2.199967\n",
      "iteration 0 / 700: loss 9671.364731\n",
      "iteration 100 / 700: loss 2.248931\n",
      "iteration 200 / 700: loss 2.244775\n",
      "iteration 300 / 700: loss 2.261475\n",
      "iteration 400 / 700: loss 2.222694\n",
      "iteration 500 / 700: loss 2.255457\n",
      "iteration 600 / 700: loss 2.246808\n",
      "iteration 0 / 700: loss 22523.859172\n",
      "iteration 100 / 700: loss 2.283456\n",
      "iteration 200 / 700: loss 2.282541\n",
      "iteration 300 / 700: loss 2.277205\n",
      "iteration 400 / 700: loss 2.279698\n",
      "iteration 500 / 700: loss 2.288416\n",
      "iteration 600 / 700: loss 2.278929\n",
      "iteration 0 / 700: loss 52081.069480\n",
      "iteration 100 / 700: loss 2.299097\n",
      "iteration 200 / 700: loss 2.292172\n",
      "iteration 300 / 700: loss 2.287605\n",
      "iteration 400 / 700: loss 2.285819\n",
      "iteration 500 / 700: loss 2.294189\n",
      "iteration 600 / 700: loss 2.297397\n",
      "iteration 0 / 700: loss 121803.370078\n",
      "iteration 100 / 700: loss 2.302549\n",
      "iteration 200 / 700: loss 2.301608\n",
      "iteration 300 / 700: loss 2.301586\n",
      "iteration 400 / 700: loss 2.303629\n",
      "iteration 500 / 700: loss 2.306369\n",
      "iteration 600 / 700: loss 2.301900\n",
      "iteration 0 / 700: loss 285710.057586\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 655700.521678\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1533993.631865\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 769.757325\n",
      "iteration 100 / 700: loss 166.845086\n",
      "iteration 200 / 700: loss 37.525682\n",
      "iteration 300 / 700: loss 9.675463\n",
      "iteration 400 / 700: loss 3.713184\n",
      "iteration 500 / 700: loss 2.437799\n",
      "iteration 600 / 700: loss 2.155757\n",
      "iteration 0 / 700: loss 1801.902209\n",
      "iteration 100 / 700: loss 51.776328\n",
      "iteration 200 / 700: loss 3.498109\n",
      "iteration 300 / 700: loss 2.144540\n",
      "iteration 400 / 700: loss 2.154952\n",
      "iteration 500 / 700: loss 2.127539\n",
      "iteration 600 / 700: loss 2.162055\n",
      "iteration 0 / 700: loss 4188.096507\n",
      "iteration 100 / 700: loss 3.117962\n",
      "iteration 200 / 700: loss 2.172210\n",
      "iteration 300 / 700: loss 2.200919\n",
      "iteration 400 / 700: loss 2.231394\n",
      "iteration 500 / 700: loss 2.217746\n",
      "iteration 600 / 700: loss 2.215036\n",
      "iteration 0 / 700: loss 9744.520281\n",
      "iteration 100 / 700: loss 2.255834\n",
      "iteration 200 / 700: loss 2.254391\n",
      "iteration 300 / 700: loss 2.244018\n",
      "iteration 400 / 700: loss 2.273561\n",
      "iteration 500 / 700: loss 2.252838\n",
      "iteration 600 / 700: loss 2.250498\n",
      "iteration 0 / 700: loss 22443.499330\n",
      "iteration 100 / 700: loss 2.290338\n",
      "iteration 200 / 700: loss 2.285504\n",
      "iteration 300 / 700: loss 2.280862\n",
      "iteration 400 / 700: loss 2.279509\n",
      "iteration 500 / 700: loss 2.275661\n",
      "iteration 600 / 700: loss 2.284882\n",
      "iteration 0 / 700: loss 51626.147360\n",
      "iteration 100 / 700: loss 2.291210\n",
      "iteration 200 / 700: loss 2.291384\n",
      "iteration 300 / 700: loss 2.299275\n",
      "iteration 400 / 700: loss 2.291992\n",
      "iteration 500 / 700: loss 2.293626\n",
      "iteration 600 / 700: loss 2.292116\n",
      "iteration 0 / 700: loss 122203.119192\n",
      "iteration 100 / 700: loss 2.304729\n",
      "iteration 200 / 700: loss 2.305679\n",
      "iteration 300 / 700: loss 2.304517\n",
      "iteration 400 / 700: loss 2.301970\n",
      "iteration 500 / 700: loss 2.302581\n",
      "iteration 600 / 700: loss 2.302519\n",
      "iteration 0 / 700: loss 282993.167635\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 658019.653315\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1528289.771249\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 780.067211\n",
      "iteration 100 / 700: loss 147.967029\n",
      "iteration 200 / 700: loss 29.484405\n",
      "iteration 300 / 700: loss 7.190516\n",
      "iteration 400 / 700: loss 3.023045\n",
      "iteration 500 / 700: loss 2.310777\n",
      "iteration 600 / 700: loss 2.160629\n",
      "iteration 0 / 700: loss 1807.101895\n",
      "iteration 100 / 700: loss 38.261970\n",
      "iteration 200 / 700: loss 2.872893\n",
      "iteration 300 / 700: loss 2.212956\n",
      "iteration 400 / 700: loss 2.131852\n",
      "iteration 500 / 700: loss 2.174130\n",
      "iteration 600 / 700: loss 2.199161\n",
      "iteration 0 / 700: loss 4155.749189\n",
      "iteration 100 / 700: loss 2.611797\n",
      "iteration 200 / 700: loss 2.221099\n",
      "iteration 300 / 700: loss 2.216421\n",
      "iteration 400 / 700: loss 2.211797\n",
      "iteration 500 / 700: loss 2.170109\n",
      "iteration 600 / 700: loss 2.207004\n",
      "iteration 0 / 700: loss 9737.174083\n",
      "iteration 100 / 700: loss 2.245002\n",
      "iteration 200 / 700: loss 2.241410\n",
      "iteration 300 / 700: loss 2.245202\n",
      "iteration 400 / 700: loss 2.254506\n",
      "iteration 500 / 700: loss 2.244501\n",
      "iteration 600 / 700: loss 2.246557\n",
      "iteration 0 / 700: loss 22653.212755\n",
      "iteration 100 / 700: loss 2.280967\n",
      "iteration 200 / 700: loss 2.281451\n",
      "iteration 300 / 700: loss 2.285090\n",
      "iteration 400 / 700: loss 2.275743\n",
      "iteration 500 / 700: loss 2.278205\n",
      "iteration 600 / 700: loss 2.287454\n",
      "iteration 0 / 700: loss 52488.852563\n",
      "iteration 100 / 700: loss 2.295966\n",
      "iteration 200 / 700: loss 2.290408\n",
      "iteration 300 / 700: loss 2.292020\n",
      "iteration 400 / 700: loss 2.296959\n",
      "iteration 500 / 700: loss 2.295456\n",
      "iteration 600 / 700: loss 2.291232\n",
      "iteration 0 / 700: loss 121676.439040\n",
      "iteration 100 / 700: loss 2.304646\n",
      "iteration 200 / 700: loss 2.303213\n",
      "iteration 300 / 700: loss 2.306200\n",
      "iteration 400 / 700: loss 2.300584\n",
      "iteration 500 / 700: loss 2.306699\n",
      "iteration 600 / 700: loss 2.306776\n",
      "iteration 0 / 700: loss 281749.523074\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 651522.805647\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1546100.528553\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 773.671120\n",
      "iteration 100 / 700: loss 126.652755\n",
      "iteration 200 / 700: loss 22.257027\n",
      "iteration 300 / 700: loss 5.381528\n",
      "iteration 400 / 700: loss 2.594278\n",
      "iteration 500 / 700: loss 2.250155\n",
      "iteration 600 / 700: loss 2.098190\n",
      "iteration 0 / 700: loss 1761.923534\n",
      "iteration 100 / 700: loss 27.016677\n",
      "iteration 200 / 700: loss 2.525748\n",
      "iteration 300 / 700: loss 2.124578\n",
      "iteration 400 / 700: loss 2.149746\n",
      "iteration 500 / 700: loss 2.177623\n",
      "iteration 600 / 700: loss 2.157057\n",
      "iteration 0 / 700: loss 4196.833332\n",
      "iteration 100 / 700: loss 2.400504\n",
      "iteration 200 / 700: loss 2.213712\n",
      "iteration 300 / 700: loss 2.195999\n",
      "iteration 400 / 700: loss 2.231111\n",
      "iteration 500 / 700: loss 2.205041\n",
      "iteration 600 / 700: loss 2.209755\n",
      "iteration 0 / 700: loss 9645.787324\n",
      "iteration 100 / 700: loss 2.277912\n",
      "iteration 200 / 700: loss 2.250704\n",
      "iteration 300 / 700: loss 2.261235\n",
      "iteration 400 / 700: loss 2.250810\n",
      "iteration 500 / 700: loss 2.255326\n",
      "iteration 600 / 700: loss 2.243233\n",
      "iteration 0 / 700: loss 22633.179342\n",
      "iteration 100 / 700: loss 2.288706\n",
      "iteration 200 / 700: loss 2.278412\n",
      "iteration 300 / 700: loss 2.276637\n",
      "iteration 400 / 700: loss 2.284517\n",
      "iteration 500 / 700: loss 2.273529\n",
      "iteration 600 / 700: loss 2.292470\n",
      "iteration 0 / 700: loss 52267.327024\n",
      "iteration 100 / 700: loss 2.294973\n",
      "iteration 200 / 700: loss 2.295073\n",
      "iteration 300 / 700: loss 2.292053\n",
      "iteration 400 / 700: loss 2.297614\n",
      "iteration 500 / 700: loss 2.293058\n",
      "iteration 600 / 700: loss 2.293153\n",
      "iteration 0 / 700: loss 122724.566584\n",
      "iteration 100 / 700: loss 2.304329\n",
      "iteration 200 / 700: loss 2.307055\n",
      "iteration 300 / 700: loss 2.309196\n",
      "iteration 400 / 700: loss 2.304725\n",
      "iteration 500 / 700: loss 2.302666\n",
      "iteration 600 / 700: loss 2.302378\n",
      "iteration 0 / 700: loss 280547.760594\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 662961.262966\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1555079.126632\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 772.963441\n",
      "iteration 100 / 700: loss 107.796538\n",
      "iteration 200 / 700: loss 16.612413\n",
      "iteration 300 / 700: loss 4.133012\n",
      "iteration 400 / 700: loss 2.285918\n",
      "iteration 500 / 700: loss 2.172393\n",
      "iteration 600 / 700: loss 2.116845\n",
      "iteration 0 / 700: loss 1780.549664\n",
      "iteration 100 / 700: loss 19.302373\n",
      "iteration 200 / 700: loss 2.328044\n",
      "iteration 300 / 700: loss 2.128010\n",
      "iteration 400 / 700: loss 2.159804\n",
      "iteration 500 / 700: loss 2.151685\n",
      "iteration 600 / 700: loss 2.166824\n",
      "iteration 0 / 700: loss 4128.435149\n",
      "iteration 100 / 700: loss 2.303352\n",
      "iteration 200 / 700: loss 2.240718\n",
      "iteration 300 / 700: loss 2.210954\n",
      "iteration 400 / 700: loss 2.213342\n",
      "iteration 500 / 700: loss 2.209044\n",
      "iteration 600 / 700: loss 2.203620\n",
      "iteration 0 / 700: loss 9687.638601\n",
      "iteration 100 / 700: loss 2.270248\n",
      "iteration 200 / 700: loss 2.241018\n",
      "iteration 300 / 700: loss 2.253065\n",
      "iteration 400 / 700: loss 2.236497\n",
      "iteration 500 / 700: loss 2.272594\n",
      "iteration 600 / 700: loss 2.273380\n",
      "iteration 0 / 700: loss 22395.162138\n",
      "iteration 100 / 700: loss 2.285567\n",
      "iteration 200 / 700: loss 2.283139\n",
      "iteration 300 / 700: loss 2.284018\n",
      "iteration 400 / 700: loss 2.289656\n",
      "iteration 500 / 700: loss 2.271087\n",
      "iteration 600 / 700: loss 2.268874\n",
      "iteration 0 / 700: loss 52114.611675\n",
      "iteration 100 / 700: loss 2.292723\n",
      "iteration 200 / 700: loss 2.293294\n",
      "iteration 300 / 700: loss 2.294791\n",
      "iteration 400 / 700: loss 2.295228\n",
      "iteration 500 / 700: loss 2.286248\n",
      "iteration 600 / 700: loss 2.296875\n",
      "iteration 0 / 700: loss 121863.122236\n",
      "iteration 100 / 700: loss 2.308972\n",
      "iteration 200 / 700: loss 2.310829\n",
      "iteration 300 / 700: loss 2.313127\n",
      "iteration 400 / 700: loss 2.313608\n",
      "iteration 500 / 700: loss 2.308166\n",
      "iteration 600 / 700: loss 2.315692\n",
      "iteration 0 / 700: loss 283574.728183\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 660443.206796\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1532545.991574\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 778.360608\n",
      "iteration 100 / 700: loss 91.338517\n",
      "iteration 200 / 700: loss 12.392773\n",
      "iteration 300 / 700: loss 3.359089\n",
      "iteration 400 / 700: loss 2.273061\n",
      "iteration 500 / 700: loss 2.105430\n",
      "iteration 600 / 700: loss 2.058072\n",
      "iteration 0 / 700: loss 1780.658212\n",
      "iteration 100 / 700: loss 13.455544\n",
      "iteration 200 / 700: loss 2.179191\n",
      "iteration 300 / 700: loss 2.157552\n",
      "iteration 400 / 700: loss 2.193295\n",
      "iteration 500 / 700: loss 2.194481\n",
      "iteration 600 / 700: loss 2.187756\n",
      "iteration 0 / 700: loss 4180.010467\n",
      "iteration 100 / 700: loss 2.247613\n",
      "iteration 200 / 700: loss 2.253463\n",
      "iteration 300 / 700: loss 2.206511\n",
      "iteration 400 / 700: loss 2.210972\n",
      "iteration 500 / 700: loss 2.209171\n",
      "iteration 600 / 700: loss 2.210497\n",
      "iteration 0 / 700: loss 9706.252085\n",
      "iteration 100 / 700: loss 2.241625\n",
      "iteration 200 / 700: loss 2.254940\n",
      "iteration 300 / 700: loss 2.264912\n",
      "iteration 400 / 700: loss 2.246724\n",
      "iteration 500 / 700: loss 2.252767\n",
      "iteration 600 / 700: loss 2.243850\n",
      "iteration 0 / 700: loss 22750.972704\n",
      "iteration 100 / 700: loss 2.274782\n",
      "iteration 200 / 700: loss 2.277332\n",
      "iteration 300 / 700: loss 2.292915\n",
      "iteration 400 / 700: loss 2.283219\n",
      "iteration 500 / 700: loss 2.271734\n",
      "iteration 600 / 700: loss 2.272571\n",
      "iteration 0 / 700: loss 52470.680608\n",
      "iteration 100 / 700: loss 2.295921\n",
      "iteration 200 / 700: loss 2.300120\n",
      "iteration 300 / 700: loss 2.296500\n",
      "iteration 400 / 700: loss 2.299052\n",
      "iteration 500 / 700: loss 2.299939\n",
      "iteration 600 / 700: loss 2.293116\n",
      "iteration 0 / 700: loss 123462.679289\n",
      "iteration 100 / 700: loss 2.320987\n",
      "iteration 200 / 700: loss 2.316350\n",
      "iteration 300 / 700: loss 2.322128\n",
      "iteration 400 / 700: loss 2.314904\n",
      "iteration 500 / 700: loss 2.324417\n",
      "iteration 600 / 700: loss 2.314900\n",
      "iteration 0 / 700: loss 283518.798024\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 662318.929610\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1547962.099565\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 773.694252\n",
      "iteration 100 / 700: loss 75.370598\n",
      "iteration 200 / 700: loss 9.110888\n",
      "iteration 300 / 700: loss 2.791048\n",
      "iteration 400 / 700: loss 2.086405\n",
      "iteration 500 / 700: loss 2.143137\n",
      "iteration 600 / 700: loss 2.073873\n",
      "iteration 0 / 700: loss 1790.616026\n",
      "iteration 100 / 700: loss 9.417426\n",
      "iteration 200 / 700: loss 2.239637\n",
      "iteration 300 / 700: loss 2.153169\n",
      "iteration 400 / 700: loss 2.171469\n",
      "iteration 500 / 700: loss 2.150787\n",
      "iteration 600 / 700: loss 2.146336\n",
      "iteration 0 / 700: loss 4208.089638\n",
      "iteration 100 / 700: loss 2.233763\n",
      "iteration 200 / 700: loss 2.244210\n",
      "iteration 300 / 700: loss 2.236748\n",
      "iteration 400 / 700: loss 2.185137\n",
      "iteration 500 / 700: loss 2.197107\n",
      "iteration 600 / 700: loss 2.209668\n",
      "iteration 0 / 700: loss 9645.005732\n",
      "iteration 100 / 700: loss 2.269206\n",
      "iteration 200 / 700: loss 2.250355\n",
      "iteration 300 / 700: loss 2.266947\n",
      "iteration 400 / 700: loss 2.232464\n",
      "iteration 500 / 700: loss 2.254507\n",
      "iteration 600 / 700: loss 2.261409\n",
      "iteration 0 / 700: loss 22309.156974\n",
      "iteration 100 / 700: loss 2.281509\n",
      "iteration 200 / 700: loss 2.277521\n",
      "iteration 300 / 700: loss 2.289914\n",
      "iteration 400 / 700: loss 2.278826\n",
      "iteration 500 / 700: loss 2.273283\n",
      "iteration 600 / 700: loss 2.287728\n",
      "iteration 0 / 700: loss 51679.099930\n",
      "iteration 100 / 700: loss 2.301519\n",
      "iteration 200 / 700: loss 2.298177\n",
      "iteration 300 / 700: loss 2.294932\n",
      "iteration 400 / 700: loss 2.293771\n",
      "iteration 500 / 700: loss 2.299901\n",
      "iteration 600 / 700: loss 2.295007\n",
      "iteration 0 / 700: loss 120906.009720\n",
      "iteration 100 / 700: loss 2.342574\n",
      "iteration 200 / 700: loss 2.355418\n",
      "iteration 300 / 700: loss 2.357695\n",
      "iteration 400 / 700: loss 2.359305\n",
      "iteration 500 / 700: loss 2.354364\n",
      "iteration 600 / 700: loss 2.343638\n",
      "iteration 0 / 700: loss 285807.312111\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 657060.499314\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1532283.900836\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 775.665117\n",
      "iteration 100 / 700: loss 61.738027\n",
      "iteration 200 / 700: loss 6.734694\n",
      "iteration 300 / 700: loss 2.475467\n",
      "iteration 400 / 700: loss 2.175959\n",
      "iteration 500 / 700: loss 2.088104\n",
      "iteration 600 / 700: loss 2.091248\n",
      "iteration 0 / 700: loss 1796.243873\n",
      "iteration 100 / 700: loss 6.564498\n",
      "iteration 200 / 700: loss 2.149456\n",
      "iteration 300 / 700: loss 2.142993\n",
      "iteration 400 / 700: loss 2.129461\n",
      "iteration 500 / 700: loss 2.179412\n",
      "iteration 600 / 700: loss 2.137778\n",
      "iteration 0 / 700: loss 4115.984307\n",
      "iteration 100 / 700: loss 2.215955\n",
      "iteration 200 / 700: loss 2.233728\n",
      "iteration 300 / 700: loss 2.202621\n",
      "iteration 400 / 700: loss 2.194063\n",
      "iteration 500 / 700: loss 2.208201\n",
      "iteration 600 / 700: loss 2.224759\n",
      "iteration 0 / 700: loss 9688.429331\n",
      "iteration 100 / 700: loss 2.257694\n",
      "iteration 200 / 700: loss 2.252962\n",
      "iteration 300 / 700: loss 2.256067\n",
      "iteration 400 / 700: loss 2.244355\n",
      "iteration 500 / 700: loss 2.271943\n",
      "iteration 600 / 700: loss 2.272076\n",
      "iteration 0 / 700: loss 22026.943740\n",
      "iteration 100 / 700: loss 2.273636\n",
      "iteration 200 / 700: loss 2.271989\n",
      "iteration 300 / 700: loss 2.290156\n",
      "iteration 400 / 700: loss 2.285764\n",
      "iteration 500 / 700: loss 2.282570\n",
      "iteration 600 / 700: loss 2.284256\n",
      "iteration 0 / 700: loss 51938.336492\n",
      "iteration 100 / 700: loss 2.299650\n",
      "iteration 200 / 700: loss 2.300050\n",
      "iteration 300 / 700: loss 2.297654\n",
      "iteration 400 / 700: loss 2.291158\n",
      "iteration 500 / 700: loss 2.298009\n",
      "iteration 600 / 700: loss 2.301990\n",
      "iteration 0 / 700: loss 122752.735645\n",
      "iteration 100 / 700: loss 2538429.664310\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 279823.191887\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 649625.388993\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1528453.566708\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 777.102129\n",
      "iteration 100 / 700: loss 49.617226\n",
      "iteration 200 / 700: loss 5.011477\n",
      "iteration 300 / 700: loss 2.215493\n",
      "iteration 400 / 700: loss 2.080289\n",
      "iteration 500 / 700: loss 2.052413\n",
      "iteration 600 / 700: loss 2.082303\n",
      "iteration 0 / 700: loss 1804.593380\n",
      "iteration 100 / 700: loss 4.723118\n",
      "iteration 200 / 700: loss 2.145845\n",
      "iteration 300 / 700: loss 2.137993\n",
      "iteration 400 / 700: loss 2.153806\n",
      "iteration 500 / 700: loss 2.177903\n",
      "iteration 600 / 700: loss 2.118239\n",
      "iteration 0 / 700: loss 4104.771405\n",
      "iteration 100 / 700: loss 2.211587\n",
      "iteration 200 / 700: loss 2.247085\n",
      "iteration 300 / 700: loss 2.239540\n",
      "iteration 400 / 700: loss 2.228867\n",
      "iteration 500 / 700: loss 2.227595\n",
      "iteration 600 / 700: loss 2.210267\n",
      "iteration 0 / 700: loss 9783.037144\n",
      "iteration 100 / 700: loss 2.260442\n",
      "iteration 200 / 700: loss 2.272723\n",
      "iteration 300 / 700: loss 2.253148\n",
      "iteration 400 / 700: loss 2.275129\n",
      "iteration 500 / 700: loss 2.266080\n",
      "iteration 600 / 700: loss 2.265878\n",
      "iteration 0 / 700: loss 22449.954413\n",
      "iteration 100 / 700: loss 2.283700\n",
      "iteration 200 / 700: loss 2.276178\n",
      "iteration 300 / 700: loss 2.292640\n",
      "iteration 400 / 700: loss 2.280020\n",
      "iteration 500 / 700: loss 2.275374\n",
      "iteration 600 / 700: loss 2.281218\n",
      "iteration 0 / 700: loss 53146.795587\n",
      "iteration 100 / 700: loss 2.298493\n",
      "iteration 200 / 700: loss 2.303850\n",
      "iteration 300 / 700: loss 2.295335\n",
      "iteration 400 / 700: loss 2.297306\n",
      "iteration 500 / 700: loss 2.295320\n",
      "iteration 600 / 700: loss 2.300429\n",
      "iteration 0 / 700: loss 122593.844398\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 280205.996972\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 666665.456554\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1529087.262015\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 763.365303\n",
      "iteration 100 / 700: loss 38.535045\n",
      "iteration 200 / 700: loss 3.881689\n",
      "iteration 300 / 700: loss 2.198518\n",
      "iteration 400 / 700: loss 2.116973\n",
      "iteration 500 / 700: loss 2.069708\n",
      "iteration 600 / 700: loss 2.127643\n",
      "iteration 0 / 700: loss 1786.316813\n",
      "iteration 100 / 700: loss 3.620073\n",
      "iteration 200 / 700: loss 2.211384\n",
      "iteration 300 / 700: loss 2.138598\n",
      "iteration 400 / 700: loss 2.123950\n",
      "iteration 500 / 700: loss 2.123677\n",
      "iteration 600 / 700: loss 2.153092\n",
      "iteration 0 / 700: loss 4110.331631\n",
      "iteration 100 / 700: loss 2.218440\n",
      "iteration 200 / 700: loss 2.240684\n",
      "iteration 300 / 700: loss 2.196469\n",
      "iteration 400 / 700: loss 2.224365\n",
      "iteration 500 / 700: loss 2.228955\n",
      "iteration 600 / 700: loss 2.204908\n",
      "iteration 0 / 700: loss 9481.381679\n",
      "iteration 100 / 700: loss 2.252439\n",
      "iteration 200 / 700: loss 2.261845\n",
      "iteration 300 / 700: loss 2.250655\n",
      "iteration 400 / 700: loss 2.267374\n",
      "iteration 500 / 700: loss 2.264834\n",
      "iteration 600 / 700: loss 2.253913\n",
      "iteration 0 / 700: loss 22552.492337\n",
      "iteration 100 / 700: loss 2.285549\n",
      "iteration 200 / 700: loss 2.286347\n",
      "iteration 300 / 700: loss 2.281663\n",
      "iteration 400 / 700: loss 2.292819\n",
      "iteration 500 / 700: loss 2.287876\n",
      "iteration 600 / 700: loss 2.272128\n",
      "iteration 0 / 700: loss 53564.174356\n",
      "iteration 100 / 700: loss 2.298431\n",
      "iteration 200 / 700: loss 2.299562\n",
      "iteration 300 / 700: loss 2.303942\n",
      "iteration 400 / 700: loss 2.305986\n",
      "iteration 500 / 700: loss 2.302509\n",
      "iteration 600 / 700: loss 2.294939\n",
      "iteration 0 / 700: loss 120191.343304\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 285338.868710\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 657012.951734\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1527446.361336\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 778.714048\n",
      "iteration 100 / 700: loss 30.469765\n",
      "iteration 200 / 700: loss 3.109087\n",
      "iteration 300 / 700: loss 2.120340\n",
      "iteration 400 / 700: loss 2.154982\n",
      "iteration 500 / 700: loss 2.083344\n",
      "iteration 600 / 700: loss 2.047842\n",
      "iteration 0 / 700: loss 1786.396092\n",
      "iteration 100 / 700: loss 2.928563\n",
      "iteration 200 / 700: loss 2.147206\n",
      "iteration 300 / 700: loss 2.161681\n",
      "iteration 400 / 700: loss 2.173385\n",
      "iteration 500 / 700: loss 2.140405\n",
      "iteration 600 / 700: loss 2.230203\n",
      "iteration 0 / 700: loss 4192.512790\n",
      "iteration 100 / 700: loss 2.211963\n",
      "iteration 200 / 700: loss 2.183992\n",
      "iteration 300 / 700: loss 2.230487\n",
      "iteration 400 / 700: loss 2.210799\n",
      "iteration 500 / 700: loss 2.194409\n",
      "iteration 600 / 700: loss 2.185675\n",
      "iteration 0 / 700: loss 9666.608007\n",
      "iteration 100 / 700: loss 2.223227\n",
      "iteration 200 / 700: loss 2.261580\n",
      "iteration 300 / 700: loss 2.262499\n",
      "iteration 400 / 700: loss 2.247291\n",
      "iteration 500 / 700: loss 2.253308\n",
      "iteration 600 / 700: loss 2.253045\n",
      "iteration 0 / 700: loss 22415.162843\n",
      "iteration 100 / 700: loss 2.291728\n",
      "iteration 200 / 700: loss 2.289173\n",
      "iteration 300 / 700: loss 2.293511\n",
      "iteration 400 / 700: loss 2.279357\n",
      "iteration 500 / 700: loss 2.291442\n",
      "iteration 600 / 700: loss 2.282120\n",
      "iteration 0 / 700: loss 51973.538648\n",
      "iteration 100 / 700: loss 2.300256\n",
      "iteration 200 / 700: loss 2.300036\n",
      "iteration 300 / 700: loss 2.307449\n",
      "iteration 400 / 700: loss 2.296174\n",
      "iteration 500 / 700: loss 2.301334\n",
      "iteration 600 / 700: loss 2.307138\n",
      "iteration 0 / 700: loss 120603.981553\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 280938.210479\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 649823.430540\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1560757.057548\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 764.812853\n",
      "iteration 100 / 700: loss 22.768607\n",
      "iteration 200 / 700: loss 2.610409\n",
      "iteration 300 / 700: loss 2.056817\n",
      "iteration 400 / 700: loss 2.091253\n",
      "iteration 500 / 700: loss 2.105938\n",
      "iteration 600 / 700: loss 2.109595\n",
      "iteration 0 / 700: loss 1810.856728\n",
      "iteration 100 / 700: loss 2.548702\n",
      "iteration 200 / 700: loss 2.142809\n",
      "iteration 300 / 700: loss 2.154372\n",
      "iteration 400 / 700: loss 2.176664\n",
      "iteration 500 / 700: loss 2.156901\n",
      "iteration 600 / 700: loss 2.150443\n",
      "iteration 0 / 700: loss 4189.702870\n",
      "iteration 100 / 700: loss 2.236158\n",
      "iteration 200 / 700: loss 2.203544\n",
      "iteration 300 / 700: loss 2.206183\n",
      "iteration 400 / 700: loss 2.222806\n",
      "iteration 500 / 700: loss 2.204075\n",
      "iteration 600 / 700: loss 2.206212\n",
      "iteration 0 / 700: loss 9821.198657\n",
      "iteration 100 / 700: loss 2.260356\n",
      "iteration 200 / 700: loss 2.270372\n",
      "iteration 300 / 700: loss 2.234049\n",
      "iteration 400 / 700: loss 2.268972\n",
      "iteration 500 / 700: loss 2.271445\n",
      "iteration 600 / 700: loss 2.284203\n",
      "iteration 0 / 700: loss 22439.233215\n",
      "iteration 100 / 700: loss 2.273143\n",
      "iteration 200 / 700: loss 2.281089\n",
      "iteration 300 / 700: loss 2.285846\n",
      "iteration 400 / 700: loss 2.265000\n",
      "iteration 500 / 700: loss 2.290485\n",
      "iteration 600 / 700: loss 2.289085\n",
      "iteration 0 / 700: loss 52237.761857\n",
      "iteration 100 / 700: loss 2.297705\n",
      "iteration 200 / 700: loss 2.307749\n",
      "iteration 300 / 700: loss 2.299468\n",
      "iteration 400 / 700: loss 2.296406\n",
      "iteration 500 / 700: loss 2.301319\n",
      "iteration 600 / 700: loss 2.308477\n",
      "iteration 0 / 700: loss 119920.445709\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 286000.206548\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 654964.544569\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1546876.550641\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 781.363432\n",
      "iteration 100 / 700: loss 17.403751\n",
      "iteration 200 / 700: loss 2.408567\n",
      "iteration 300 / 700: loss 2.106174\n",
      "iteration 400 / 700: loss 2.006015\n",
      "iteration 500 / 700: loss 2.075972\n",
      "iteration 600 / 700: loss 2.058775\n",
      "iteration 0 / 700: loss 1776.497381\n",
      "iteration 100 / 700: loss 2.310470\n",
      "iteration 200 / 700: loss 2.161261\n",
      "iteration 300 / 700: loss 2.208033\n",
      "iteration 400 / 700: loss 2.218712\n",
      "iteration 500 / 700: loss 2.143744\n",
      "iteration 600 / 700: loss 2.170967\n",
      "iteration 0 / 700: loss 4165.007682\n",
      "iteration 100 / 700: loss 2.210869\n",
      "iteration 200 / 700: loss 2.196483\n",
      "iteration 300 / 700: loss 2.215360\n",
      "iteration 400 / 700: loss 2.199528\n",
      "iteration 500 / 700: loss 2.235313\n",
      "iteration 600 / 700: loss 2.219181\n",
      "iteration 0 / 700: loss 9631.153556\n",
      "iteration 100 / 700: loss 2.264235\n",
      "iteration 200 / 700: loss 2.232005\n",
      "iteration 300 / 700: loss 2.262246\n",
      "iteration 400 / 700: loss 2.253444\n",
      "iteration 500 / 700: loss 2.260580\n",
      "iteration 600 / 700: loss 2.264547\n",
      "iteration 0 / 700: loss 22364.059025\n",
      "iteration 100 / 700: loss 2.286681\n",
      "iteration 200 / 700: loss 2.280736\n",
      "iteration 300 / 700: loss 2.278367\n",
      "iteration 400 / 700: loss 2.285802\n",
      "iteration 500 / 700: loss 2.267351\n",
      "iteration 600 / 700: loss 2.284515\n",
      "iteration 0 / 700: loss 52784.620082\n",
      "iteration 100 / 700: loss 2.300073\n",
      "iteration 200 / 700: loss 2.302955\n",
      "iteration 300 / 700: loss 2.310817\n",
      "iteration 400 / 700: loss 2.310912\n",
      "iteration 500 / 700: loss 2.307199\n",
      "iteration 600 / 700: loss 2.309586\n",
      "iteration 0 / 700: loss 122292.999507\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 283403.169872\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 665923.175778\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1536066.558894\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 774.803469\n",
      "iteration 100 / 700: loss 12.844937\n",
      "iteration 200 / 700: loss 2.226882\n",
      "iteration 300 / 700: loss 2.111416\n",
      "iteration 400 / 700: loss 2.063847\n",
      "iteration 500 / 700: loss 2.077059\n",
      "iteration 600 / 700: loss 2.079058\n",
      "iteration 0 / 700: loss 1784.436712\n",
      "iteration 100 / 700: loss 2.227795\n",
      "iteration 200 / 700: loss 2.133017\n",
      "iteration 300 / 700: loss 2.122114\n",
      "iteration 400 / 700: loss 2.153400\n",
      "iteration 500 / 700: loss 2.136107\n",
      "iteration 600 / 700: loss 2.162002\n",
      "iteration 0 / 700: loss 4215.677905\n",
      "iteration 100 / 700: loss 2.207032\n",
      "iteration 200 / 700: loss 2.206490\n",
      "iteration 300 / 700: loss 2.236867\n",
      "iteration 400 / 700: loss 2.194370\n",
      "iteration 500 / 700: loss 2.231188\n",
      "iteration 600 / 700: loss 2.225816\n",
      "iteration 0 / 700: loss 9637.530646\n",
      "iteration 100 / 700: loss 2.274355\n",
      "iteration 200 / 700: loss 2.265392\n",
      "iteration 300 / 700: loss 2.271686\n",
      "iteration 400 / 700: loss 2.243911\n",
      "iteration 500 / 700: loss 2.252331\n",
      "iteration 600 / 700: loss 2.272519\n",
      "iteration 0 / 700: loss 22462.717327\n",
      "iteration 100 / 700: loss 2.286343\n",
      "iteration 200 / 700: loss 2.297196\n",
      "iteration 300 / 700: loss 2.286501\n",
      "iteration 400 / 700: loss 2.263062\n",
      "iteration 500 / 700: loss 2.287463\n",
      "iteration 600 / 700: loss 2.289697\n",
      "iteration 0 / 700: loss 52399.551063\n",
      "iteration 100 / 700: loss 2.312718\n",
      "iteration 200 / 700: loss 2.319850\n",
      "iteration 300 / 700: loss 2.316910\n",
      "iteration 400 / 700: loss 2.311311\n",
      "iteration 500 / 700: loss 2.315515\n",
      "iteration 600 / 700: loss 2.319888\n",
      "iteration 0 / 700: loss 121727.460210\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 283566.259885\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 655256.740622\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1541674.862889\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 769.203195\n",
      "iteration 100 / 700: loss 9.319576\n",
      "iteration 200 / 700: loss 2.146567\n",
      "iteration 300 / 700: loss 2.077758\n",
      "iteration 400 / 700: loss 2.091910\n",
      "iteration 500 / 700: loss 2.125767\n",
      "iteration 600 / 700: loss 2.095874\n",
      "iteration 0 / 700: loss 1773.811398\n",
      "iteration 100 / 700: loss 2.189332\n",
      "iteration 200 / 700: loss 2.146006\n",
      "iteration 300 / 700: loss 2.148761\n",
      "iteration 400 / 700: loss 2.136650\n",
      "iteration 500 / 700: loss 2.186294\n",
      "iteration 600 / 700: loss 2.150369\n",
      "iteration 0 / 700: loss 4133.551909\n",
      "iteration 100 / 700: loss 2.222041\n",
      "iteration 200 / 700: loss 2.221590\n",
      "iteration 300 / 700: loss 2.222592\n",
      "iteration 400 / 700: loss 2.194473\n",
      "iteration 500 / 700: loss 2.197429\n",
      "iteration 600 / 700: loss 2.215455\n",
      "iteration 0 / 700: loss 9713.739550\n",
      "iteration 100 / 700: loss 2.271768\n",
      "iteration 200 / 700: loss 2.264100\n",
      "iteration 300 / 700: loss 2.263666\n",
      "iteration 400 / 700: loss 2.264257\n",
      "iteration 500 / 700: loss 2.240705\n",
      "iteration 600 / 700: loss 2.270838\n",
      "iteration 0 / 700: loss 22612.329843\n",
      "iteration 100 / 700: loss 2.288487\n",
      "iteration 200 / 700: loss 2.284165\n",
      "iteration 300 / 700: loss 2.292883\n",
      "iteration 400 / 700: loss 2.278351\n",
      "iteration 500 / 700: loss 2.289313\n",
      "iteration 600 / 700: loss 2.287267\n",
      "iteration 0 / 700: loss 52725.752703\n",
      "iteration 100 / 700: loss 2.312843\n",
      "iteration 200 / 700: loss 2.322280\n",
      "iteration 300 / 700: loss 2.337186\n",
      "iteration 400 / 700: loss 2.316947\n",
      "iteration 500 / 700: loss 2.335240\n",
      "iteration 600 / 700: loss 2.338544\n",
      "iteration 0 / 700: loss 121977.557682\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 285625.727176\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 662630.661245\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1551009.976416\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 778.801103\n",
      "iteration 100 / 700: loss 6.926632\n",
      "iteration 200 / 700: loss 2.120346\n",
      "iteration 300 / 700: loss 2.127725\n",
      "iteration 400 / 700: loss 2.077576\n",
      "iteration 500 / 700: loss 2.073403\n",
      "iteration 600 / 700: loss 2.069271\n",
      "iteration 0 / 700: loss 1774.283490\n",
      "iteration 100 / 700: loss 2.151490\n",
      "iteration 200 / 700: loss 2.137695\n",
      "iteration 300 / 700: loss 2.178125\n",
      "iteration 400 / 700: loss 2.129909\n",
      "iteration 500 / 700: loss 2.205712\n",
      "iteration 600 / 700: loss 2.144365\n",
      "iteration 0 / 700: loss 4184.373318\n",
      "iteration 100 / 700: loss 2.201773\n",
      "iteration 200 / 700: loss 2.213364\n",
      "iteration 300 / 700: loss 2.192147\n",
      "iteration 400 / 700: loss 2.181212\n",
      "iteration 500 / 700: loss 2.219647\n",
      "iteration 600 / 700: loss 2.234529\n",
      "iteration 0 / 700: loss 9708.730346\n",
      "iteration 100 / 700: loss 2.273334\n",
      "iteration 200 / 700: loss 2.251248\n",
      "iteration 300 / 700: loss 2.276310\n",
      "iteration 400 / 700: loss 2.261947\n",
      "iteration 500 / 700: loss 2.268198\n",
      "iteration 600 / 700: loss 2.271282\n",
      "iteration 0 / 700: loss 22424.760480\n",
      "iteration 100 / 700: loss 2.290347\n",
      "iteration 200 / 700: loss 2.277121\n",
      "iteration 300 / 700: loss 2.280106\n",
      "iteration 400 / 700: loss 2.285774\n",
      "iteration 500 / 700: loss 2.304061\n",
      "iteration 600 / 700: loss 2.288842\n",
      "iteration 0 / 700: loss 52353.524267\n",
      "iteration 100 / 700: loss 2.378789\n",
      "iteration 200 / 700: loss 2.397686\n",
      "iteration 300 / 700: loss 2.363107\n",
      "iteration 400 / 700: loss 2.432615\n",
      "iteration 500 / 700: loss 2.365074\n",
      "iteration 600 / 700: loss 2.344349\n",
      "iteration 0 / 700: loss 122998.517769\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 283760.538959\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 651590.382878\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "iteration 0 / 700: loss 1552252.369137\n",
      "iteration 100 / 700: loss nan\n",
      "iteration 200 / 700: loss nan\n",
      "iteration 300 / 700: loss nan\n",
      "iteration 400 / 700: loss nan\n",
      "iteration 500 / 700: loss nan\n",
      "iteration 600 / 700: loss nan\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.326918 val accuracy: 0.339000\n",
      "lr 1.000000e-07 reg 1.163459e+05 train accuracy: 0.308347 val accuracy: 0.318000\n",
      "lr 1.000000e-07 reg 2.707274e+05 train accuracy: 0.278041 val accuracy: 0.289000\n",
      "lr 1.000000e-07 reg 6.299605e+05 train accuracy: 0.258388 val accuracy: 0.266000\n",
      "lr 1.000000e-07 reg 1.465867e+06 train accuracy: 0.249082 val accuracy: 0.262000\n",
      "lr 1.000000e-07 reg 3.410952e+06 train accuracy: 0.254408 val accuracy: 0.271000\n",
      "lr 1.000000e-07 reg 7.937005e+06 train accuracy: 0.219755 val accuracy: 0.235000\n",
      "lr 1.000000e-07 reg 1.846876e+07 train accuracy: 0.171000 val accuracy: 0.156000\n",
      "lr 1.000000e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.088398e-07 reg 5.000000e+04 train accuracy: 0.330612 val accuracy: 0.349000\n",
      "lr 1.088398e-07 reg 1.163459e+05 train accuracy: 0.308776 val accuracy: 0.318000\n",
      "lr 1.088398e-07 reg 2.707274e+05 train accuracy: 0.279673 val accuracy: 0.293000\n",
      "lr 1.088398e-07 reg 6.299605e+05 train accuracy: 0.253429 val accuracy: 0.264000\n",
      "lr 1.088398e-07 reg 1.465867e+06 train accuracy: 0.268429 val accuracy: 0.274000\n",
      "lr 1.088398e-07 reg 3.410952e+06 train accuracy: 0.231776 val accuracy: 0.246000\n",
      "lr 1.088398e-07 reg 7.937005e+06 train accuracy: 0.210163 val accuracy: 0.221000\n",
      "lr 1.088398e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.088398e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.088398e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.184611e-07 reg 5.000000e+04 train accuracy: 0.330633 val accuracy: 0.338000\n",
      "lr 1.184611e-07 reg 1.163459e+05 train accuracy: 0.309878 val accuracy: 0.319000\n",
      "lr 1.184611e-07 reg 2.707274e+05 train accuracy: 0.263449 val accuracy: 0.273000\n",
      "lr 1.184611e-07 reg 6.299605e+05 train accuracy: 0.260082 val accuracy: 0.265000\n",
      "lr 1.184611e-07 reg 1.465867e+06 train accuracy: 0.252531 val accuracy: 0.256000\n",
      "lr 1.184611e-07 reg 3.410952e+06 train accuracy: 0.252510 val accuracy: 0.261000\n",
      "lr 1.184611e-07 reg 7.937005e+06 train accuracy: 0.233408 val accuracy: 0.229000\n",
      "lr 1.184611e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.184611e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.184611e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.289329e-07 reg 5.000000e+04 train accuracy: 0.332143 val accuracy: 0.342000\n",
      "lr 1.289329e-07 reg 1.163459e+05 train accuracy: 0.300531 val accuracy: 0.317000\n",
      "lr 1.289329e-07 reg 2.707274e+05 train accuracy: 0.282347 val accuracy: 0.290000\n",
      "lr 1.289329e-07 reg 6.299605e+05 train accuracy: 0.272918 val accuracy: 0.285000\n",
      "lr 1.289329e-07 reg 1.465867e+06 train accuracy: 0.240510 val accuracy: 0.252000\n",
      "lr 1.289329e-07 reg 3.410952e+06 train accuracy: 0.245816 val accuracy: 0.252000\n",
      "lr 1.289329e-07 reg 7.937005e+06 train accuracy: 0.194673 val accuracy: 0.201000\n",
      "lr 1.289329e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.289329e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.289329e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.403303e-07 reg 5.000000e+04 train accuracy: 0.328163 val accuracy: 0.340000\n",
      "lr 1.403303e-07 reg 1.163459e+05 train accuracy: 0.304429 val accuracy: 0.322000\n",
      "lr 1.403303e-07 reg 2.707274e+05 train accuracy: 0.287531 val accuracy: 0.307000\n",
      "lr 1.403303e-07 reg 6.299605e+05 train accuracy: 0.253429 val accuracy: 0.271000\n",
      "lr 1.403303e-07 reg 1.465867e+06 train accuracy: 0.249469 val accuracy: 0.263000\n",
      "lr 1.403303e-07 reg 3.410952e+06 train accuracy: 0.251633 val accuracy: 0.268000\n",
      "lr 1.403303e-07 reg 7.937005e+06 train accuracy: 0.200082 val accuracy: 0.215000\n",
      "lr 1.403303e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.403303e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.403303e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.527353e-07 reg 5.000000e+04 train accuracy: 0.329327 val accuracy: 0.345000\n",
      "lr 1.527353e-07 reg 1.163459e+05 train accuracy: 0.299531 val accuracy: 0.306000\n",
      "lr 1.527353e-07 reg 2.707274e+05 train accuracy: 0.283653 val accuracy: 0.284000\n",
      "lr 1.527353e-07 reg 6.299605e+05 train accuracy: 0.259469 val accuracy: 0.271000\n",
      "lr 1.527353e-07 reg 1.465867e+06 train accuracy: 0.240714 val accuracy: 0.249000\n",
      "lr 1.527353e-07 reg 3.410952e+06 train accuracy: 0.238429 val accuracy: 0.239000\n",
      "lr 1.527353e-07 reg 7.937005e+06 train accuracy: 0.205020 val accuracy: 0.215000\n",
      "lr 1.527353e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.527353e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.527353e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.662369e-07 reg 5.000000e+04 train accuracy: 0.326776 val accuracy: 0.349000\n",
      "lr 1.662369e-07 reg 1.163459e+05 train accuracy: 0.303388 val accuracy: 0.324000\n",
      "lr 1.662369e-07 reg 2.707274e+05 train accuracy: 0.269408 val accuracy: 0.288000\n",
      "lr 1.662369e-07 reg 6.299605e+05 train accuracy: 0.270469 val accuracy: 0.282000\n",
      "lr 1.662369e-07 reg 1.465867e+06 train accuracy: 0.243245 val accuracy: 0.248000\n",
      "lr 1.662369e-07 reg 3.410952e+06 train accuracy: 0.224673 val accuracy: 0.240000\n",
      "lr 1.662369e-07 reg 7.937005e+06 train accuracy: 0.222735 val accuracy: 0.221000\n",
      "lr 1.662369e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.662369e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.662369e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.809320e-07 reg 5.000000e+04 train accuracy: 0.329408 val accuracy: 0.341000\n",
      "lr 1.809320e-07 reg 1.163459e+05 train accuracy: 0.298735 val accuracy: 0.304000\n",
      "lr 1.809320e-07 reg 2.707274e+05 train accuracy: 0.287898 val accuracy: 0.299000\n",
      "lr 1.809320e-07 reg 6.299605e+05 train accuracy: 0.261429 val accuracy: 0.277000\n",
      "lr 1.809320e-07 reg 1.465867e+06 train accuracy: 0.235286 val accuracy: 0.236000\n",
      "lr 1.809320e-07 reg 3.410952e+06 train accuracy: 0.237000 val accuracy: 0.257000\n",
      "lr 1.809320e-07 reg 7.937005e+06 train accuracy: 0.214673 val accuracy: 0.220000\n",
      "lr 1.809320e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.809320e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.809320e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.969260e-07 reg 5.000000e+04 train accuracy: 0.322714 val accuracy: 0.340000\n",
      "lr 1.969260e-07 reg 1.163459e+05 train accuracy: 0.295490 val accuracy: 0.302000\n",
      "lr 1.969260e-07 reg 2.707274e+05 train accuracy: 0.282306 val accuracy: 0.309000\n",
      "lr 1.969260e-07 reg 6.299605e+05 train accuracy: 0.266327 val accuracy: 0.290000\n",
      "lr 1.969260e-07 reg 1.465867e+06 train accuracy: 0.254653 val accuracy: 0.267000\n",
      "lr 1.969260e-07 reg 3.410952e+06 train accuracy: 0.224224 val accuracy: 0.211000\n",
      "lr 1.969260e-07 reg 7.937005e+06 train accuracy: 0.154878 val accuracy: 0.158000\n",
      "lr 1.969260e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.969260e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.969260e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.143340e-07 reg 5.000000e+04 train accuracy: 0.321163 val accuracy: 0.332000\n",
      "lr 2.143340e-07 reg 1.163459e+05 train accuracy: 0.302980 val accuracy: 0.312000\n",
      "lr 2.143340e-07 reg 2.707274e+05 train accuracy: 0.273816 val accuracy: 0.286000\n",
      "lr 2.143340e-07 reg 6.299605e+05 train accuracy: 0.247000 val accuracy: 0.265000\n",
      "lr 2.143340e-07 reg 1.465867e+06 train accuracy: 0.239122 val accuracy: 0.250000\n",
      "lr 2.143340e-07 reg 3.410952e+06 train accuracy: 0.209776 val accuracy: 0.220000\n",
      "lr 2.143340e-07 reg 7.937005e+06 train accuracy: 0.200612 val accuracy: 0.204000\n",
      "lr 2.143340e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.143340e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.143340e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.332808e-07 reg 5.000000e+04 train accuracy: 0.330408 val accuracy: 0.345000\n",
      "lr 2.332808e-07 reg 1.163459e+05 train accuracy: 0.298204 val accuracy: 0.311000\n",
      "lr 2.332808e-07 reg 2.707274e+05 train accuracy: 0.288469 val accuracy: 0.298000\n",
      "lr 2.332808e-07 reg 6.299605e+05 train accuracy: 0.263469 val accuracy: 0.264000\n",
      "lr 2.332808e-07 reg 1.465867e+06 train accuracy: 0.242816 val accuracy: 0.256000\n",
      "lr 2.332808e-07 reg 3.410952e+06 train accuracy: 0.206551 val accuracy: 0.197000\n",
      "lr 2.332808e-07 reg 7.937005e+06 train accuracy: 0.204592 val accuracy: 0.196000\n",
      "lr 2.332808e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.332808e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.332808e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.539024e-07 reg 5.000000e+04 train accuracy: 0.328980 val accuracy: 0.343000\n",
      "lr 2.539024e-07 reg 1.163459e+05 train accuracy: 0.304939 val accuracy: 0.311000\n",
      "lr 2.539024e-07 reg 2.707274e+05 train accuracy: 0.271796 val accuracy: 0.289000\n",
      "lr 2.539024e-07 reg 6.299605e+05 train accuracy: 0.254918 val accuracy: 0.276000\n",
      "lr 2.539024e-07 reg 1.465867e+06 train accuracy: 0.215776 val accuracy: 0.229000\n",
      "lr 2.539024e-07 reg 3.410952e+06 train accuracy: 0.209714 val accuracy: 0.211000\n",
      "lr 2.539024e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.539024e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.539024e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.539024e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.763470e-07 reg 5.000000e+04 train accuracy: 0.333245 val accuracy: 0.336000\n",
      "lr 2.763470e-07 reg 1.163459e+05 train accuracy: 0.289735 val accuracy: 0.300000\n",
      "lr 2.763470e-07 reg 2.707274e+05 train accuracy: 0.277551 val accuracy: 0.294000\n",
      "lr 2.763470e-07 reg 6.299605e+05 train accuracy: 0.235735 val accuracy: 0.241000\n",
      "lr 2.763470e-07 reg 1.465867e+06 train accuracy: 0.255000 val accuracy: 0.257000\n",
      "lr 2.763470e-07 reg 3.410952e+06 train accuracy: 0.232469 val accuracy: 0.238000\n",
      "lr 2.763470e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.763470e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.763470e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.763470e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.007756e-07 reg 5.000000e+04 train accuracy: 0.329041 val accuracy: 0.338000\n",
      "lr 3.007756e-07 reg 1.163459e+05 train accuracy: 0.299612 val accuracy: 0.308000\n",
      "lr 3.007756e-07 reg 2.707274e+05 train accuracy: 0.268041 val accuracy: 0.281000\n",
      "lr 3.007756e-07 reg 6.299605e+05 train accuracy: 0.249551 val accuracy: 0.269000\n",
      "lr 3.007756e-07 reg 1.465867e+06 train accuracy: 0.239714 val accuracy: 0.246000\n",
      "lr 3.007756e-07 reg 3.410952e+06 train accuracy: 0.200245 val accuracy: 0.219000\n",
      "lr 3.007756e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.007756e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.007756e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.007756e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.273637e-07 reg 5.000000e+04 train accuracy: 0.325306 val accuracy: 0.339000\n",
      "lr 3.273637e-07 reg 1.163459e+05 train accuracy: 0.303347 val accuracy: 0.316000\n",
      "lr 3.273637e-07 reg 2.707274e+05 train accuracy: 0.276102 val accuracy: 0.285000\n",
      "lr 3.273637e-07 reg 6.299605e+05 train accuracy: 0.254367 val accuracy: 0.262000\n",
      "lr 3.273637e-07 reg 1.465867e+06 train accuracy: 0.251245 val accuracy: 0.265000\n",
      "lr 3.273637e-07 reg 3.410952e+06 train accuracy: 0.219571 val accuracy: 0.218000\n",
      "lr 3.273637e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.273637e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.273637e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.273637e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.563021e-07 reg 5.000000e+04 train accuracy: 0.320265 val accuracy: 0.330000\n",
      "lr 3.563021e-07 reg 1.163459e+05 train accuracy: 0.286020 val accuracy: 0.303000\n",
      "lr 3.563021e-07 reg 2.707274e+05 train accuracy: 0.279000 val accuracy: 0.279000\n",
      "lr 3.563021e-07 reg 6.299605e+05 train accuracy: 0.268327 val accuracy: 0.281000\n",
      "lr 3.563021e-07 reg 1.465867e+06 train accuracy: 0.247633 val accuracy: 0.250000\n",
      "lr 3.563021e-07 reg 3.410952e+06 train accuracy: 0.220857 val accuracy: 0.206000\n",
      "lr 3.563021e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.563021e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.563021e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.563021e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.877987e-07 reg 5.000000e+04 train accuracy: 0.318694 val accuracy: 0.337000\n",
      "lr 3.877987e-07 reg 1.163459e+05 train accuracy: 0.285939 val accuracy: 0.310000\n",
      "lr 3.877987e-07 reg 2.707274e+05 train accuracy: 0.289959 val accuracy: 0.292000\n",
      "lr 3.877987e-07 reg 6.299605e+05 train accuracy: 0.264224 val accuracy: 0.271000\n",
      "lr 3.877987e-07 reg 1.465867e+06 train accuracy: 0.241347 val accuracy: 0.258000\n",
      "lr 3.877987e-07 reg 3.410952e+06 train accuracy: 0.195306 val accuracy: 0.183000\n",
      "lr 3.877987e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.877987e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.877987e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.877987e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.220795e-07 reg 5.000000e+04 train accuracy: 0.328041 val accuracy: 0.338000\n",
      "lr 4.220795e-07 reg 1.163459e+05 train accuracy: 0.304245 val accuracy: 0.314000\n",
      "lr 4.220795e-07 reg 2.707274e+05 train accuracy: 0.278367 val accuracy: 0.284000\n",
      "lr 4.220795e-07 reg 6.299605e+05 train accuracy: 0.270531 val accuracy: 0.277000\n",
      "lr 4.220795e-07 reg 1.465867e+06 train accuracy: 0.239918 val accuracy: 0.256000\n",
      "lr 4.220795e-07 reg 3.410952e+06 train accuracy: 0.156531 val accuracy: 0.155000\n",
      "lr 4.220795e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.220795e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.220795e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.220795e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.593906e-07 reg 5.000000e+04 train accuracy: 0.328959 val accuracy: 0.354000\n",
      "lr 4.593906e-07 reg 1.163459e+05 train accuracy: 0.296224 val accuracy: 0.306000\n",
      "lr 4.593906e-07 reg 2.707274e+05 train accuracy: 0.255878 val accuracy: 0.272000\n",
      "lr 4.593906e-07 reg 6.299605e+05 train accuracy: 0.241367 val accuracy: 0.254000\n",
      "lr 4.593906e-07 reg 1.465867e+06 train accuracy: 0.209449 val accuracy: 0.215000\n",
      "lr 4.593906e-07 reg 3.410952e+06 train accuracy: 0.208653 val accuracy: 0.204000\n",
      "lr 4.593906e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.593906e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.593906e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.593906e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.325041 val accuracy: 0.345000\n",
      "lr 5.000000e-07 reg 1.163459e+05 train accuracy: 0.310265 val accuracy: 0.323000\n",
      "lr 5.000000e-07 reg 2.707274e+05 train accuracy: 0.264429 val accuracy: 0.270000\n",
      "lr 5.000000e-07 reg 6.299605e+05 train accuracy: 0.244224 val accuracy: 0.264000\n",
      "lr 5.000000e-07 reg 1.465867e+06 train accuracy: 0.226592 val accuracy: 0.237000\n",
      "lr 5.000000e-07 reg 3.410952e+06 train accuracy: 0.185490 val accuracy: 0.162000\n",
      "lr 5.000000e-07 reg 7.937005e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 1.846876e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 4.297530e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.354000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:74: RuntimeWarning: overflow encountered in exp\n",
      "  num_pixel = X.shape[1]\n",
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:75: RuntimeWarning: overflow encountered in exp\n",
      "  num_classes = W.shape[1]\n",
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:75: RuntimeWarning: invalid value encountered in true_divide\n",
      "  num_classes = W.shape[1]\n",
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  num_classes = W.shape[1]\n",
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:77: RuntimeWarning: overflow encountered in exp\n",
      "  total_exp = np.sum(np.exp(scores), 1)\n",
      "/Users/twer/workspace/cs231n_assignment1/cs231n/classifiers/softmax.py:77: RuntimeWarning: invalid value encountered in true_divide\n",
      "  total_exp = np.sum(np.exp(scores), 1)\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# DONE:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for learning_rate in np.logspace(np.log10(learning_rates[0]), np.log10(learning_rates[1]), 20):\n",
    "    for regularization_strength in np.logspace(np.log10(regularization_strengths[0]), np.log10(regularization_strengths[1]), 10):\n",
    "        softmax = Softmax()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=learning_rate, reg=regularization_strength,\n",
    "                              num_iters=700, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(learning_rate, regularization_strength)] = (train_accuracy, val_accuracy)\n",
    "        if val_accuracy > best_val:\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.326000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5MtTIpFIJBKJxBP4H9DeuDkXXXTwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf4f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
